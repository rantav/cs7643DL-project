{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b2eb969f-aa40-4711-8eac-cfcd36694ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time, os, copy, argparse\n",
    "import multiprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "00f30589-fe84-4d0d-951c-2960c4abcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = 'D:/datasets/afhq/train'\n",
    "valid_directory = 'D:/datasets/afhq/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6cb319de-c58e-49ce-87f8-81af3247712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    " \n",
    "# Size of train and validation data\n",
    "dataset_sizes = {\n",
    "    'train':len(dataset['train']),\n",
    "    'valid':len(dataset['valid'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "beb502e3-d245-460b-a088-d0e85373d77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['cat', 'dog', 'wild']\n",
      "Training-set size: 14630 \n",
      "Validation-set size: 1500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataloaders = {\n",
    "    'train':data.DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True),\n",
    "    'valid':data.DataLoader(dataset['valid'], batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "}\n",
    "# Class names or target labels\n",
    "class_names = dataset['train'].classes\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Print the train and validation data sizes\n",
    "print(\"Training-set size:\",dataset_sizes['train'],\n",
    "      \"\\nValidation-set size:\", dataset_sizes['valid'])\n",
    "# Set default device as gpu, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8745c8fb-565e-41a5-8688-6a63c88a3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained models\n",
    "model_ft = models.resnet18().to(device=device)\n",
    "# Modify fully connected layers to match num_classes\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "model_ft.fc = nn.Linear(num_ftrs, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bfabc2aa-da61-4138-b2be-65cfb32b4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# print('Model Summary:-\\n')\n",
    "# for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "#     print(num, name, param.requires_grad )\n",
    "# summary(model_ft, input_size=(3, 224, 224))\n",
    "# print(model_ft)\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d2427d2b-8f93-4f16-9e9a-1f9013017f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer \n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "# Learning rate decay\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d688e9d-ad7e-4e3f-9423-1b81f1099d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:-\n",
      "\n",
      "Epoch 0/9\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Model training routine \n",
    "print(\"\\nTraining:-\\n\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs).to(device=device)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "num_epochs = 10\n",
    "%time model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=num_epochs)\n",
    "# Save the model\n",
    "PATH=\"saved-models/model_1.pth\" \n",
    "print(\"\\nSaving the model...\")\n",
    "torch.save(model_ft, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8767442-53aa-40b4-97b9-dc52084f66b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved-models/model_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [98], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m EVAL_MODEL\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msaved-models/model_1.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(EVAL_MODEL)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    577\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    580\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    581\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    582\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    583\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved-models/model_1.pth'"
     ]
    }
   ],
   "source": [
    "EVAL_MODEL= 'saved-models/model_1.pth'\n",
    "model = torch.load(EVAL_MODEL)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a983f1-75b3-4947-954d-4af9a438754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import multiprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "EVAL_DIR = '/Users/rantavory/tmp/work/testing'\n",
    "\n",
    "\n",
    "# Prepare the eval data loader\n",
    "eval_transform=transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])])\n",
    "eval_dataset = datasets.ImageFolder(root=EVAL_DIR, transform=eval_transform)\n",
    "eval_loader = data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# Enable gpu mode, if cuda available\n",
    "# Number of classes and dataset-size\n",
    "num_classes=len(eval_dataset.classes)\n",
    "dsize=len(eval_dataset)\n",
    "# Class label names\n",
    "class_names=['Cezanne', 'Degas', 'Gauguin', 'Hassam', 'Matisse', 'Monet', 'Pissarro', 'Renoir', 'Sargent', 'VanGogh']\n",
    "# Initialize the prediction and label lists\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "# Evaluate the model accuracy on the dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in eval_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).to(device)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        predlist=torch.cat([predlist,predicted.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,labels.view(-1).cpu()])\n",
    "# Overall accuracy\n",
    "overall_accuracy=100 * correct / total\n",
    "print('Accuracy of the network on the {:d} test images: {:.2f}%'.format(dsize, \n",
    "    overall_accuracy))\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(conf_mat,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c5081-8647-4ca5-b5db-d65380c3fb54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "851ccbde2705a1e4b2857de158061d763b50afc2e73e08dd379440b9d82f578d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
